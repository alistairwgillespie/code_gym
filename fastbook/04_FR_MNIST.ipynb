{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Study\n",
    "# Further Research on the full MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('training'),Path('testing')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) [Path('training/9'),Path('training/0'),Path('training/7'),Path('training/6'),Path('training/1'),Path('training/8'),Path('training/4'),Path('training/3'),Path('training/2'),Path('training/5')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'training').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) [Path('testing/9'),Path('testing/0'),Path('testing/7'),Path('testing/6'),Path('testing/1'),Path('testing/8'),Path('testing/4'),Path('testing/3'),Path('testing/2'),Path('testing/5')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'testing').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peek at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = (path/'training'/'3').ls().sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_tensors = [tensor(Image.open(o)) for o in threes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_tensors[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(three_tensors[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, when working with floats we want the data to be between 0 and 1 thus we will divide each by 255 to do this. Moreover, we need to stack our images into a nice rank 3 tensor for our model. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_threes = torch.stack(three_tensors).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6131, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_threes.size() # 6131 images of 28 pixel width and 28 pixel length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_threes.ndim # rank 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(num, data='training'):\n",
    "    nums = (path/data/num).ls().sorted()\n",
    "    num_tensors = [tensor(Image.open(o)) for o in nums]\n",
    "    return torch.stack(num_tensors).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8118, 0.9922],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3294, 0.9804, 0.9804],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3098, 0.8824, 0.9882, 0.7882],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3804, 0.9882, 0.9882, 0.4745],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.5804, 0.9882, 0.5569, 0.7176],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.8510, 0.9882, 0.9686, 0.9922],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.8549, 0.9922, 0.9922, 1.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.2392, 0.9294, 0.9882, 0.9882, 0.9451]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data('8')[0][8:16, 8:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = [get_data(str(i)) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.cat(training).view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for n, x in enumerate(training):\n",
    "    labels += [n]*len(x)\n",
    "train_y = tensor(labels).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]), torch.Size([60000, 1]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60000 images, 28*28 (784) pixels\n",
    "# 60000 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].shape # first digit image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = [get_data(str(i), data='testing') for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = torch.cat(testing).view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for n, x in enumerate(testing):\n",
    "    labels += [n]*len(x)\n",
    "valid_y = tensor(labels).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 784]), torch.Size([10000, 1]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_x: 10000 images, 28*28 (784) pixels\n",
    "# valid_y: 10000 labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emulating the PyTorch dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = list(zip(train_x,train_y))\n",
    "valid_dset = list(zip(valid_x,valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(train_dset, batch_size=256)\n",
    "xb,yb = first(dl) # Take a sample for example\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = DataLoader(valid_dset, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialise weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((28*28,10))\n",
    "bias = init_params(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0342, -0.4174,  1.6537,  ...,  2.6324, -0.8313, -0.0899],\n",
       "         [-3.1966,  1.5680,  0.6664,  ...,  0.5375,  0.7464, -0.6315],\n",
       "         [-0.6763, -1.0859, -0.0695,  ..., -0.0945, -0.5653,  0.5952],\n",
       "         ...,\n",
       "         [-0.6856,  0.3372, -0.2091,  ..., -0.3588,  0.3843, -2.8089],\n",
       "         [ 0.1603,  0.1264,  0.3964,  ...,  0.1280,  0.3844,  0.5591],\n",
       "         [-1.1435, -2.2296,  0.4682,  ..., -0.2936, -0.1988, -1.2676]], requires_grad=True),\n",
       " tensor([-0.4231,  0.2452, -0.7060, -1.6251, -0.6217,  0.3191, -0.0889,  2.3983,  1.4903,  0.6836], requires_grad=True))"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 784]), torch.Size([4, 1]))"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x, batch_y = xb[:4], yb[:4]\n",
    "batch_x.shape, batch_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of the box with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_linear = nn.Linear(784, 10, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 784]),\n",
       " tensor(-0.0001, grad_fn=<MeanBackward0>),\n",
       " Parameter containing:\n",
       " tensor([-0.0166,  0.0081,  0.0302,  0.0149,  0.0028,  0.0105, -0.0246,  0.0174, -0.0041,  0.0193], requires_grad=True))"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_linear.weight.shape, t_linear.weight.mean(), t_linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_output = t_linear(batch_x)\n",
    "t_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3060, -0.4787, -0.0137,  0.1627, -0.2323,  0.1287,  0.2014,  0.0201, -0.1759,  0.1103], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_preds = torch.argmax(t_output, dim=1)\n",
    "t_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 6\n",
      "Actual: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAJ2klEQVR4nO2bW28b1RaAP1/GHie2E7uO4zi1naSxcWPHkEQglQBVUCsoEo9IvCAknvgRPPMPeAP+QBESispDC1KhaqA0bVqXS+qkSZM6jvG92PF9POehx3MaU3rLJC1H/iQrUmzP7PmyZq+1155oZFmmy//QPusBPG90hXTQFdJBV0gHXSEd6B/x/v9zCtI86JfdCOmgK6SDrpAOukI66ArpoCukg0el3QNHkiQkSaJWq1Gv1ymXyzQaDUwmE0ajEVEU0ev16PV6tFr1/57PnZBKpcJff/3FlStXWFxc5KeffiIajXLixAkCgQBvvfUWXq8Xm82GKIqqn/+5EdKOjFQqRSwWY2lpiZWVFW7dukUmk2F1dRUAh8NBqVRiZmZmX4Qgy/LDXgdGqVSSE4mE/Omnn8o+n0+22+2yyWSSBUGQdTqdbDAYZJPJJLtcLjkYDMpLS0t7PeUDr/mZR0iz2aTZbPLnn3+ysrLC6uoquVyORqNBo9FAq9Wi0+mU+aJUKtFqtSgWi1SrVQRBQKvVotE8sBJ/Yp65kGq1Sj6f5+uvv+bLL78knU6zs7MDgEajwWAwYDAYEAQBjUZDsVikWCyytraG1+tlYGAAo9GompQDF9IOTUmSaDabJJNJfvvtN2KxGLlcjkqlsuvzoihiNpsZGxvD4XBw6dIlkskk169fRxAEZmZmsNvt9PX1YTAY9jy+ZyJEkiTK5TLZbJb5+Xk+//xzMpkM2WwWuaPHa7fb8Xg8vPvuu0xPT1Or1Thz5gyfffYZRqORjz/+mOnpad5+++1/p5CdnR2SySTb29ssLy8TjUYfGBltSqUSqVSKRqMBQF9fH4ODgxQKBWq1GoVCgXQ6jSRJqozvwIXE43G++OILfv31V3744QckSaLVav3j51OplBI99Xodl8vFK6+8wqVLl8hkMqRSKTY3N6nX66qM78CENJtNGo0G6XSa27dvs7W1tesirFYrFouFQ4cOYbfbuXnzJtvb2xiNRgwGA319ffT39+P3+zEajdy8eZN0Or0rZarBgQlpNBpks1lu3LjBhQsXKJfLu953Op2Ew2EmJiYIBoOcPn2a+fl5TCYTVquVw4cPMzQ0hMlkIhwOs7i4SCwWo9VqqSYDDkCIJEnU63W2trY4f/48S0tLVCoVms0mAP39/dhsNo4fP86rr76KxWLBbDYTCAR48803GR8fZ2hoiMnJSSwWC4IgYDabsVqtyLJMPB5HFEUKhQI2m01JwU/LvgtpNBrk83l+/PFHPvnkE8rlshIdGo0Gt9vN1NQU77//PrOzs2SzWVKpFHNzc7zwwgu8/vrrDA8PK7WI1Wql2WzS398PQCwWI5lMsra2Rn9/Pw6HY0/ZZt+ESJKkyPjjjz+IxWKUy2VarRYmkwlBEDAajQSDQcLhMA6HA71ej8ViQavVYrVa8Xg8OBwOBEFAp9Mpx76/AGs2m9RqNX7//XcsFgtWq/X5FbKzs8Pm5ibz8/MsLy+zs7ODyWRSQt5utzMxMcEbb7yBy+VCp9NhsViwWCyPfZ5ms0mlUuHy5cvkcjkmJiYwm81PPe59EdJqtSiVSiwuLhKNRolGo0rGCIVCvPbaa9jtdgYHB/H7/Tidzj2tXNtrm2KxuOd6RHUh7RSYTqc5ffo0KysrLCwsoNfrEUWRmZkZPvzwQyW9tlotJEnCaDTu6Zz5fJ58Pv/8CWmHcHuNsr29jSzLBINBTp48yezsLMPDwxiNRvR6/b0lt16/a454lqguRJIkcrkcKysrXLt2TSm5X3zxRT766COcTid2u13t06qG6kKKxSLnzp3j6tWrSJKExWLB5XIxOjqKy+Xa063RiSzLSsZ5WPn/JKgupFAo8O2333Lr1i1arRa9vb1MTEwwPj5OX1+fao2c+8v1zp97QTUh9Xqdu3fvsrGxwerqKplMBoCBgQFCoRAej0etUwH3apH2S6vVMjw8jNvtRq/f2yWpJqTZbJJIJIjFYsTjcarVKnCvn3H06FEcDodap1JoR5tOp8NmszE4OIggCHs6pmpC2o2fer1Os9lEo9FgtVrx+XxMTU3hcDj2dLvIskyr1SKRSJBMJtna2kKj0eDxeHC73bzzzjuEw2F6enr2dB2qziHthrEkSWi1Wnp6ehgYGMDtdu+5m9UWsr6+zs8//7xLyJEjR5iensbj8Tw/EdKJRqNBEARlx+1p7+325Lm1tUUikeCbb75hcXGRu3fvMjAwwOzsLC+99BI2m23P8wfs496uTqdTFnCdi7MnoV3JxuNxFhYWlFetVsPhcBAOh4lEIphMJlU67/sWIXa7ndnZWcbGxp5qkNVqlVqtxvr6OhsbG5w9e5YbN24giiJzc3O89957RCIRfD4fZrN5z7dKm30TYjab8Xq9DA4OPtH32rVEO41fu3aNy5cvc/78eVZXV5mbm8Pv93Py5ElGRkZUH7eqWeb+AimdTvPLL78wNDTE8ePHH/s4mUyGra0tFhYWuH79Omtra2xubuJyuQgEAnzwwQdEIhGcTqdaQ9/Fvqx24d72wfLyMseOHUOSpL81eP6pysxms0SjUS5cuMC5c+eoVCo0Gg0CgQCBQICpqSlGR0fVHraCqkLurx7r9Tr5fJ5oNMp3332H2+3G6/Wi0+nQ6/VUKhUKhQLb29usrKyQzWbJ5/Pcvn2b9fV1CoUCZrNZ6ZecOHFCySb7iWpCOifO9u7c2toaFy9eJBQKKe09URTJ5XIkEgmuXLnC1atXicfjbGxsUCgUKBaLOBwObDYbIyMjuN1uQqEQXq9X1cXhg9jXJnO7kPrqq6+4ePEiZ8+eRRRFTCYTmUyGO3fukMvllJ27arXKoUOHGBsbIxgM4na7OXXqFH6/X5GpVjb5J1QV0n50QafTKfNEu7UXj8eJxWIYjUZ6e3uVXbc27e/ZbDaOHDnC6OgoPp+P8fFxDh8+rOYwH4pqQgRBwOv1EolEiEQiJBIJ7ty5o7xfq9XI5XJotVr0er2ya9d+biwUCuH3+zl16hQvv/yysmPX29ur1hAfC9WEtNcudrudkZERNBoN2WxWefCl/cgU3JtvdDodRqMRq9VKf38/Y2NjTExMMDk5ic/nU2tYT4zmEU2Vx+64tBdflUpFmSC///57YrEYZ86c2dXR0mg0TE5OcuzYMUKhEFNTU0prsaenZ3+eHfs7DyyfVc0yOp0OURTxeDyIokgmk0GSJJxOp7IH285Gfr+fo0ePMj09TTAYxGQyHZSIh6JahChf+G+ktHfU6vU6pVLpb+09URTp6elBEAQMBgNarXZfnjt9CA+MENWF/Ivo/r/M49AV0kFXSAddIR10hXTQFdLBowozdfYd/0V0I6SDrpAOukI66ArpoCukg66QDv4DrGo26YvpiGsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Predicted: {preds[0]}')\n",
    "print(f'Actual: {batch_y[0].item()}')\n",
    "show_image(batch[0].view(-1, 28, 28));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear layer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear1(xb): return xb@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = linear1(batch_x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = torch.argmax(output, dim=1)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 9\n",
      "Actual: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Predicted: {preds[0]}')\n",
    "print(f'Actual: {batch_y[0].item()}')\n",
    "show_image(batch_x[0].view(-1, 28, 28));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax + NLL Loss is the way to go when it comes to having more than two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_acts = torch.softmax(t_output, dim=1) # using PyTorch\n",
    "sm_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1322, 0.0603, 0.0961, 0.1146, 0.0772, 0.1108, 0.1191, 0.0994, 0.0817, 0.1087], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_acts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import exp\n",
    "def softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sm_acts = softmax(t_output)\n",
    "my_sm_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1285, 0.1164, 0.0767, 0.0981, 0.1051, 0.1139, 0.0915, 0.0796, 0.0917, 0.0985], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_sm_acts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(sm_acts, dim=1) \n",
    "# Softmax produces a probability for each class with all activations adding to 1 for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1322, -0.1110, -0.1101, -0.1245], grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ = batch_y.squeeze()\n",
    "F.nll_loss(sm_acts, targ, reduction='none') \n",
    "# However, this is misleading given that Torch doesn't take the log for you.\n",
    "# Instead to do softmax, and true NLL, we use CrossEntropyLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLL Loss is misleading in Torch with it not actually taking the log like we need. Instead,\n",
    "# CrossEntropyLoss takes scare of all the steps for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2833, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_func(sm_acts, targ)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "# weights.grad.shape, weights.grad.mean(), bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 784]),\n",
       " tensor(-7.1275e-11),\n",
       " tensor([-0.1049,  0.0085,  0.0131,  0.0139,  0.0081,  0.0136,  0.0113,  0.0122,  0.0103,  0.0139]))"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_linear.weight.grad.shape, t_linear.weight.grad.mean(), t_linear.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 784]),\n",
       " tensor(-4.8657e-10),\n",
       " tensor([-0.9855,  0.0817,  0.1224,  0.1288,  0.0778,  0.1268,  0.1054,  0.1150,  0.0983,  0.1294]))"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_grad(batch_x, targ, t_linear)\n",
    "t_linear.weight.grad.shape, t_linear.weight.grad.mean(), t_linear.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 784]),\n",
       " tensor(-2.9194e-09),\n",
       " tensor([-1.8660,  0.1548,  0.2318,  0.2437,  0.1474,  0.2400,  0.1995,  0.2178,  0.1863,  0.2449]))"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opps the gradient have changed because we have cleared them from the last pass.\n",
    "calc_grad(batch_x, targ, t_linear)\n",
    "t_linear.weight.grad.shape, t_linear.weight.grad.mean(), t_linear.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Step the weights\n",
    "\n",
    "Putting it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_linear.weight.data -= t_linear.weight.grad * 0.0001\n",
    "t_linear.bias.data -= t_linear.bias.grad * 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_linear.weight.grad.zero_()\n",
    "t_linear.bias.grad.zero_();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, params):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        # step the weights\n",
    "        model.weight.data -= t_linear.weight.grad * 0.0001\n",
    "        model.bias.data -= t_linear.bias.grad * 0.0001\n",
    "        model.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we don't update any lingering gradients between batches, we need to zero out the currently stored gradients before we move on to the next batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. & 7. Repeat until you want to stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastbook",
   "language": "python",
   "name": "fastbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
